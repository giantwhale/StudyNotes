{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Displaying gym Game in Jupyter Notebook\n",
    "\n",
    "ref: http://mckinziebrandon.me/TensorflowNotebooks/2016/12/21/openai.html\n",
    "\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a git, with controls\n",
    "    \"\"\"\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-14 23:20:32,122] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at step 12\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(50):\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Finished at step %d\" % t)\n",
    "        break\n",
    "env.render(close=True)\n",
    "# display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Q Learning Network\n",
    "\n",
    "ref: https://keon.io/deep-q-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function as follows:\n",
    "\n",
    "$$loss=\\left(r+\\gamma \\max_a\\hat{Q}(s,a) - Q(s,a)\\right)^2$$\n",
    "\n",
    "Where $r$ is the reward, $\\gamma$ is the decay rate. The term $r+\\gamma \\max_a\\hat{Q}(s,a)$ is the **target**, and term $Q(s,a)$ is the **prediction**. In this simple problem, the Q function is a mapping:\n",
    "\n",
    "`Q: (state, action) => reward`\n",
    "\n",
    "In python, we calculate the target as:\n",
    "\n",
    "```python\n",
    "target = reward + gamma * np.max(model.predict(next_state))\n",
    "```\n",
    "\n",
    "### The Model\n",
    "\n",
    "The following model is copied from the blog. In my view this is **not the most optimal** way because it models the Q function as:\n",
    "\n",
    "`Q: state => [reward(action1), reward(action2)]`\n",
    "\n",
    "It is sub-optimal because only one of `reward(action1)` or `reward(action2)` can be observed at one time. Nonetheless, it provides us a good starting point.\n",
    "\n",
    "\n",
    "### How the Agent Decides to Act\n",
    "\n",
    "The agent will randomly select its action at first by a certain probability, called **exploration rate** or **epsilon**. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=200)\n",
    "        self.gamma = 0.95  # discount rate \n",
    "        self.epsilon = 1.0 # exploration rate \n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                Q = self.model.predict(next_state)[0]\n",
    "                target = reward + self.gamma * np.amax(Q)\n",
    "            target_f = self.model.predict(state)\n",
    "            print(target_f)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not run this block. Instead, we train the model from the terminal and retrieve it from the \n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]  # 4\n",
    "action_size = env.action_space.n  # 2\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "done = False\n",
    "batch_size = 32\n",
    "episodes = 5000\n",
    "\n",
    "for e in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, episodes, time, agent.epsilon))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "        \n",
    "agent.save('CartPole.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's play the game using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]  # 4\n",
    "action_size = env.action_space.n  # 2\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "agent.load('CartPole.model')\n",
    "\n",
    "# agent.epsilon = 1.0  # use random actions\n",
    "agent.epsilon = 0.0  # disable random actions\n",
    "\n",
    "observation = env.reset()\n",
    "state = env.reset()\n",
    "frames = []\n",
    "for t in range(5000):\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = agent.act(state.reshape((1, -1)))\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Finished at step %d\" % t)\n",
    "        break\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
